"""
í•­ê³µì‚¬ì§„ ìºì‹± ì‹œìŠ¤í…œ
ì„œë²„ ì‚¬ì´ë“œ ìºì‹±ìœ¼ë¡œ VWorld API í˜¸ì¶œ ìµœì†Œí™” ë° ì‘ë‹µ ì†ë„ í–¥ìƒ
"""

import os
import time
import json
import hashlib
from pathlib import Path
from typing import Optional, Dict, Tuple
from datetime import datetime, timedelta
import shutil


class AerialImageCache:
    """
    í•­ê³µì‚¬ì§„ ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤

    Features:
    - 24ì‹œê°„ TTL (Time To Live)
    - ìë™ ìºì‹œ ì •ë¦¬
    - ë””ìŠ¤í¬ ìš©ëŸ‰ ê´€ë¦¬
    - ìºì‹œ í†µê³„
    """

    def __init__(self, cache_dir: str = "cache/aerial_images", ttl_hours: int = 24, max_size_gb: float = 5.0):
        """
        Args:
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            ttl_hours: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì‹œê°„)
            max_size_gb: ìµœëŒ€ ìºì‹œ í¬ê¸° (GB)
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.metadata_dir = self.cache_dir / "metadata"
        self.metadata_dir.mkdir(exist_ok=True)

        self.ttl_seconds = ttl_hours * 3600
        self.max_size_bytes = int(max_size_gb * 1024 * 1024 * 1024)

        # í†µê³„ íŒŒì¼
        self.stats_file = self.cache_dir / "cache_stats.json"
        self._load_stats()

    def _load_stats(self):
        """ìºì‹œ í†µê³„ ë¡œë“œ"""
        if self.stats_file.exists():
            with open(self.stats_file, 'r') as f:
                self.stats = json.load(f)
        else:
            self.stats = {
                "hits": 0,
                "misses": 0,
                "total_requests": 0,
                "total_saved_bytes": 0,
                "last_cleanup": None
            }

    def _save_stats(self):
        """ìºì‹œ í†µê³„ ì €ì¥"""
        with open(self.stats_file, 'w') as f:
            json.dump(self.stats, f, indent=2)

    def _get_cache_key(self, latitude: float, longitude: float, zoom: int, width: int = 3, height: int = 3) -> str:
        """
        ìºì‹œ í‚¤ ìƒì„± (ì¢Œí‘œ + ì¤Œ ë ˆë²¨ + í¬ê¸°)

        Args:
            latitude: ìœ„ë„
            longitude: ê²½ë„
            zoom: ì¤Œ ë ˆë²¨
            width: íƒ€ì¼ ê°€ë¡œ ê°œìˆ˜
            height: íƒ€ì¼ ì„¸ë¡œ ê°œìˆ˜

        Returns:
            ìºì‹œ í‚¤ (í•´ì‹œê°’)
        """
        # ì¢Œí‘œë¥¼ ì†Œìˆ˜ì  4ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼ (ì•½ 11m ì •í™•ë„)
        lat_rounded = round(latitude, 4)
        lon_rounded = round(longitude, 4)

        # ìºì‹œ í‚¤ ìƒì„±
        key_str = f"{lat_rounded}_{lon_rounded}_z{zoom}_w{width}_h{height}"

        # MD5 í•´ì‹œë¡œ íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ ìƒì„±
        key_hash = hashlib.md5(key_str.encode()).hexdigest()

        return key_hash

    def get(self, latitude: float, longitude: float, zoom: int, width: int = 3, height: int = 3) -> Optional[bytes]:
        """
        ìºì‹œì—ì„œ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°

        Args:
            latitude: ìœ„ë„
            longitude: ê²½ë„
            zoom: ì¤Œ ë ˆë²¨
            width: íƒ€ì¼ ê°€ë¡œ ê°œìˆ˜
            height: íƒ€ì¼ ì„¸ë¡œ ê°œìˆ˜

        Returns:
            ì´ë¯¸ì§€ ë°ì´í„° (bytes) ë˜ëŠ” None (ìºì‹œ ë¯¸ìŠ¤)
        """
        self.stats["total_requests"] += 1

        cache_key = self._get_cache_key(latitude, longitude, zoom, width, height)
        cache_file = self.cache_dir / f"{cache_key}.jpg"
        metadata_file = self.metadata_dir / f"{cache_key}.json"

        # ìºì‹œ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not cache_file.exists():
            self.stats["misses"] += 1
            self._save_stats()
            return None

        # ìºì‹œ ë©”íƒ€ë°ì´í„° í™•ì¸
        if metadata_file.exists():
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)

            # TTL í™•ì¸
            created_at = datetime.fromisoformat(metadata['created_at'])
            age = datetime.now() - created_at

            if age.total_seconds() > self.ttl_seconds:
                # ìºì‹œ ë§Œë£Œ
                self._delete_cache_entry(cache_key)
                self.stats["misses"] += 1
                self._save_stats()
                return None

        # ìºì‹œ íˆíŠ¸!
        try:
            with open(cache_file, 'rb') as f:
                image_data = f.read()

            self.stats["hits"] += 1
            self._save_stats()

            print(f"âœ… Cache HIT: {cache_key} ({len(image_data) / 1024:.1f} KB)")
            return image_data

        except Exception as e:
            print(f"âŒ Cache read error: {e}")
            self.stats["misses"] += 1
            self._save_stats()
            return None

    def set(
        self,
        latitude: float,
        longitude: float,
        zoom: int,
        image_data: bytes,
        width: int = 3,
        height: int = 3,
        metadata: Dict = None
    ) -> bool:
        """
        ì´ë¯¸ì§€ë¥¼ ìºì‹œì— ì €ì¥

        Args:
            latitude: ìœ„ë„
            longitude: ê²½ë„
            zoom: ì¤Œ ë ˆë²¨
            image_data: ì´ë¯¸ì§€ ë°ì´í„°
            width: íƒ€ì¼ ê°€ë¡œ ê°œìˆ˜
            height: íƒ€ì¼ ì„¸ë¡œ ê°œìˆ˜
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°

        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            cache_key = self._get_cache_key(latitude, longitude, zoom, width, height)
            cache_file = self.cache_dir / f"{cache_key}.jpg"
            metadata_file = self.metadata_dir / f"{cache_key}.json"

            # ì´ë¯¸ì§€ ì €ì¥
            with open(cache_file, 'wb') as f:
                f.write(image_data)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            meta = {
                "latitude": latitude,
                "longitude": longitude,
                "zoom": zoom,
                "width": width,
                "height": height,
                "size_bytes": len(image_data),
                "created_at": datetime.now().isoformat(),
                "expires_at": (datetime.now() + timedelta(seconds=self.ttl_seconds)).isoformat()
            }

            if metadata:
                meta.update(metadata)

            with open(metadata_file, 'w') as f:
                json.dump(meta, f, indent=2)

            self.stats["total_saved_bytes"] += len(image_data)
            self._save_stats()

            print(f"ğŸ’¾ Cache SET: {cache_key} ({len(image_data) / 1024:.1f} KB)")

            # ìºì‹œ í¬ê¸° í™•ì¸ ë° ì •ë¦¬
            self._check_and_cleanup_size()

            return True

        except Exception as e:
            print(f"âŒ Cache write error: {e}")
            return False

    def _delete_cache_entry(self, cache_key: str):
        """ìºì‹œ í•­ëª© ì‚­ì œ"""
        cache_file = self.cache_dir / f"{cache_key}.jpg"
        metadata_file = self.metadata_dir / f"{cache_key}.json"

        if cache_file.exists():
            cache_file.unlink()
        if metadata_file.exists():
            metadata_file.unlink()

    def cleanup_expired(self) -> int:
        """
        ë§Œë£Œëœ ìºì‹œ ì •ë¦¬

        Returns:
            ì‚­ì œëœ ìºì‹œ ê°œìˆ˜
        """
        deleted_count = 0
        now = datetime.now()

        for metadata_file in self.metadata_dir.glob("*.json"):
            try:
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)

                created_at = datetime.fromisoformat(metadata['created_at'])
                age = now - created_at

                if age.total_seconds() > self.ttl_seconds:
                    cache_key = metadata_file.stem
                    self._delete_cache_entry(cache_key)
                    deleted_count += 1

            except Exception as e:
                print(f"âš ï¸  Error processing {metadata_file}: {e}")

        self.stats["last_cleanup"] = now.isoformat()
        self._save_stats()

        print(f"ğŸ§¹ Cleaned up {deleted_count} expired cache entries")
        return deleted_count

    def _check_and_cleanup_size(self):
        """ë””ìŠ¤í¬ ìš©ëŸ‰ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ìºì‹œë¶€í„° ì‚­ì œ"""
        total_size = self.get_cache_size()

        if total_size > self.max_size_bytes:
            print(f"âš ï¸  Cache size ({total_size / 1024 / 1024:.1f} MB) exceeds limit. Cleaning up...")

            # ëª¨ë“  ìºì‹œ í•­ëª©ì„ ìƒì„± ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            cache_entries = []
            for metadata_file in self.metadata_dir.glob("*.json"):
                try:
                    with open(metadata_file, 'r') as f:
                        metadata = json.load(f)
                    cache_entries.append((
                        datetime.fromisoformat(metadata['created_at']),
                        metadata_file.stem,
                        metadata['size_bytes']
                    ))
                except:
                    pass

            cache_entries.sort()  # ì˜¤ë˜ëœ ê²ƒë¶€í„°

            # ìš©ëŸ‰ì´ ì œí•œ ì´í•˜ë¡œ ë–¨ì–´ì§ˆ ë•Œê¹Œì§€ ì‚­ì œ
            for created_at, cache_key, size_bytes in cache_entries:
                if total_size <= self.max_size_bytes * 0.8:  # 80%ê¹Œì§€ ì¤„ì´ê¸°
                    break

                self._delete_cache_entry(cache_key)
                total_size -= size_bytes
                print(f"  Deleted old cache: {cache_key}")

    def get_cache_size(self) -> int:
        """í˜„ì¬ ìºì‹œ í¬ê¸° (bytes)"""
        total_size = 0
        for cache_file in self.cache_dir.glob("*.jpg"):
            total_size += cache_file.stat().st_size
        return total_size

    def get_stats(self) -> Dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_size = self.get_cache_size()
        cache_count = len(list(self.cache_dir.glob("*.jpg")))

        hit_rate = 0
        if self.stats["total_requests"] > 0:
            hit_rate = (self.stats["hits"] / self.stats["total_requests"]) * 100

        return {
            "total_requests": self.stats["total_requests"],
            "cache_hits": self.stats["hits"],
            "cache_misses": self.stats["misses"],
            "hit_rate_percent": round(hit_rate, 2),
            "cache_count": cache_count,
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "max_size_mb": round(self.max_size_bytes / 1024 / 1024, 2),
            "ttl_hours": self.ttl_seconds / 3600,
            "last_cleanup": self.stats["last_cleanup"]
        }

    def clear_all(self) -> int:
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        count = 0
        for cache_file in self.cache_dir.glob("*.jpg"):
            cache_file.unlink()
            count += 1

        for metadata_file in self.metadata_dir.glob("*.json"):
            metadata_file.unlink()

        self.stats = {
            "hits": 0,
            "misses": 0,
            "total_requests": 0,
            "total_saved_bytes": 0,
            "last_cleanup": datetime.now().isoformat()
        }
        self._save_stats()

        print(f"ğŸ—‘ï¸  Cleared {count} cache entries")
        return count


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_cache_instance = None

def get_cache() -> AerialImageCache:
    """ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤)"""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = AerialImageCache(
            cache_dir="cache/aerial_images",
            ttl_hours=24,  # 24ì‹œê°„ TTL
            max_size_gb=5.0  # ìµœëŒ€ 5GB
        )
    return _cache_instance


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    cache = AerialImageCache()

    print("=" * 60)
    print("í•­ê³µì‚¬ì§„ ìºì‹± ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_lat = 37.5172
    test_lon = 127.0473
    test_zoom = 18
    test_image = b"test image data" * 1000  # ê°€ì§œ ì´ë¯¸ì§€ ë°ì´í„°

    # ìºì‹œ ì €ì¥
    print("\n[í…ŒìŠ¤íŠ¸ 1] ìºì‹œ ì €ì¥")
    cache.set(test_lat, test_lon, test_zoom, test_image)

    # ìºì‹œ ì¡°íšŒ (íˆíŠ¸)
    print("\n[í…ŒìŠ¤íŠ¸ 2] ìºì‹œ ì¡°íšŒ (íˆíŠ¸)")
    result = cache.get(test_lat, test_lon, test_zoom)
    print(f"ê²°ê³¼: {'ì„±ê³µ' if result else 'ì‹¤íŒ¨'}")

    # ìºì‹œ ì¡°íšŒ (ë¯¸ìŠ¤)
    print("\n[í…ŒìŠ¤íŠ¸ 3] ìºì‹œ ì¡°íšŒ (ë¯¸ìŠ¤)")
    result = cache.get(37.5555, 127.0555, test_zoom)
    print(f"ê²°ê³¼: {'ìºì‹œ ì—†ìŒ' if not result else 'ìºì‹œ ìˆìŒ'}")

    # í†µê³„
    print("\n[í…ŒìŠ¤íŠ¸ 4] ìºì‹œ í†µê³„")
    stats = cache.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")

    print("\n" + "=" * 60)
